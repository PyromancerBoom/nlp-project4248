{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Only Dataset-v2 is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Kaggle Data: Split headlines into sarcastic and non-sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file):\n",
    "    for l in open(file,'r'):\n",
    "        yield json.loads(l)\n",
    "\n",
    "def parse_data_with_key(file, key):\n",
    "    '''\n",
    "    Output: column with title 'key'\n",
    "    '''\n",
    "    for l in open(file,'r'):\n",
    "        yield json.loads(l)[key]\n",
    "\n",
    "def parse_data_with_key_and_cond(file, key, cond_key, cond_val):\n",
    "    '''\n",
    "    Output: column with title 'key', rows where 'cond_key' column has value 'cond_val'\n",
    "    '''\n",
    "    for l in open(file,'r'):\n",
    "        row = json.loads(l)\n",
    "        if row[cond_key] == cond_val:\n",
    "            yield json.loads(l)[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '../data/Sarcasm_Headlines_Dataset_v2.json'\n",
    "sarcastic_headlines = list(parse_data_with_key_and_cond(\n",
    "    PATH, 'headline', 'is_sarcastic', 1))\n",
    "non_sarcastic_headlines = list(parse_data_with_key_and_cond(\n",
    "    PATH, 'headline', 'is_sarcastic', 0))\n",
    "\n",
    "f = open('../data/sarcastic_headlines.json', 'w')\n",
    "f.write(json.dumps(sarcastic_headlines))\n",
    "f = open('../data/non_sarcastic_headlines.json', 'w')\n",
    "f.write(json.dumps(non_sarcastic_headlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate headlines through response from LLM (Preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Response\n",
    "from json import JSONDecodeError\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13634 14985\n"
     ]
    }
   ],
   "source": [
    "# Prepare input data\n",
    "\n",
    "# Get all sarcastic headlines\n",
    "f = open('../data/sarcastic_headlines.json', 'r')\n",
    "sarcastic_headlines = json.loads(f.read())\n",
    "f.close()\n",
    "\n",
    "# Get all non-sarcastic headlines\n",
    "f = open('../data/non_sarcastic_headlines.json', 'r')\n",
    "non_sarcastic_headlines = json.loads(f.read())\n",
    "f.close()\n",
    "\n",
    "print(len(sarcastic_headlines), len(non_sarcastic_headlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate headlines through response from LLM (Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseRecorder:\n",
    "    def __init__(self, path):\n",
    "        self.final_results = []\n",
    "        self.initial_length = 0\n",
    "        self.buffer = []\n",
    "        self.path = path\n",
    "\n",
    "        try:\n",
    "            f = open(self.path, 'r')\n",
    "            data = f.read()\n",
    "            f.close()\n",
    "            self.final_results = json.loads(data)\n",
    "            self.initial_length = len(self.final_results)\n",
    "            print(\"Existing data found. Loaded successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found. Will create new file when writing.\")\n",
    "    \n",
    "    def set_buffer(self, buffer: list[str]) -> None:\n",
    "        self.buffer = buffer\n",
    "        print(\"Head of response:\")\n",
    "        print(buffer if len(buffer) < 5 else buffer[:5])\n",
    "        print(f\"Length of response: {len(buffer)}\")\n",
    "        print(\"Run \\'pop_buffer()\\' after checking response.\")\n",
    "    \n",
    "    def pop_buffer(self):\n",
    "        self.final_results = self.final_results + self.buffer\n",
    "    \n",
    "    def save(self, expected_length):\n",
    "        if self.initial_length + expected_length != len(self.final_results):\n",
    "            raise ValueError(\"Updated document has unexpected length. \"\n",
    "                \"Save Aborted. \"\n",
    "                f\"Existing file has length: {self.initial_length} \"\n",
    "                f\"Added section has length: {len(self.final_results) - self.initial_length}\")\n",
    "        f = open(self.path, 'w')\n",
    "        f.write(json.dumps(self.final_results))\n",
    "        f.close()\n",
    "\n",
    "        # Reload\n",
    "        f = open(self.path, 'r')\n",
    "        data = f.read()\n",
    "        self.final_results = json.loads(data)\n",
    "        self.initial_length = len(self.final_results)\n",
    "        self.buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(\n",
    "    llm_auth: str,\n",
    "    model: str,\n",
    "    query: str\n",
    ") -> Response:\n",
    "    response = requests.post(\n",
    "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": \"Bearer \" + llm_auth,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "        data=json.dumps({\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "            ],\n",
    "        })\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(question, headlines: list[str]) -> str:\n",
    "    return question + headlines.__str__()\n",
    "\n",
    "def split_list(l):\n",
    "    mid = len(l) // 2\n",
    "    return (l[:mid], l[mid:])\n",
    "\n",
    "def handle_response(\n",
    "    response: Response, \n",
    "    response_handler: ResponseRecorder,\n",
    "    expected_length: int\n",
    ") -> None:\n",
    "    '''\n",
    "    Parse response from online LLM and save result to local file.\n",
    "    '''\n",
    "    data = json.loads(response.content)\n",
    "    batch_result = json.loads(data['choices'][0]['message']['content'])\n",
    "    if type(batch_result) != list or len(batch_result) == 0 or type(batch_result[0]) != str:\n",
    "        raise TypeError(\"Batch result cannot be parsed to correct type or is empty.\")\n",
    "    response_handler.set_buffer(batch_result)\n",
    "    response_handler.pop_buffer()\n",
    "    response_handler.save(expected_length)\n",
    "\n",
    "def generate_headlines(\n",
    "        input_headlines: list[str],\n",
    "        save_path: str,\n",
    "        start: int,\n",
    "        end: int,\n",
    "        step: int,\n",
    "        llm_auth: str,\n",
    "        model: str,\n",
    "        question: str\n",
    "    ) -> float:\n",
    "\n",
    "    response_handler = ResponseRecorder(save_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in tqdm(range(start, end, step)):\n",
    "        if i + step >= end:\n",
    "            headline_subset = input_headlines[i:end]\n",
    "            expected_length = end - i\n",
    "        else:\n",
    "            headline_subset = input_headlines[i:i+step]\n",
    "            expected_length = step\n",
    "        \n",
    "        req_start_time = time.time()\n",
    "        try:\n",
    "            response = send_request(\n",
    "                llm_auth,\n",
    "                model,\n",
    "                get_query(question, headline_subset)\n",
    "            )\n",
    "            handle_response(response, response_handler, expected_length)\n",
    "        except JSONDecodeError:\n",
    "            # likely because the response is too long.\n",
    "            # hence break the input into 2 parts, reducing input size by half.\n",
    "            headline_subset1, headline_subset2 = split_list(headline_subset)\n",
    "            print(\"Failed to parse JSON response. Trying to reduce step size...\")\n",
    "            response1 = send_request(\n",
    "                llm_auth,\n",
    "                model,\n",
    "                get_query(question, headline_subset1)\n",
    "            )\n",
    "            handle_response(response1, response_handler, expected_length // 2)\n",
    "            response = send_request(\n",
    "                llm_auth,\n",
    "                model,\n",
    "                get_query(question, headline_subset2)\n",
    "            )\n",
    "            handle_response(response, response_handler, expected_length - expected_length // 2)\n",
    "            \n",
    "        req_end_time = time.time()\n",
    "        print(f\"Request took {(req_end_time - req_start_time):.2f} seconds.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "def get_output_file_length(save_path):\n",
    "    '''\n",
    "    For verification purpose only. To implement logic, \n",
    "    use ResponseRecorder class instead.\n",
    "    '''\n",
    "    try:\n",
    "        f = open(save_path, 'r')\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "    data = f.read()\n",
    "    output = json.loads(data)\n",
    "    return len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate headlines through response from LLM (Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your own OpenRouter API auth key\n",
    "llm_auth = \"change-to-your-own-auth-key\"\n",
    "\n",
    "# See more models on https://openrouter.ai/models\n",
    "model = \"deepseek/deepseek-chat:free\"\n",
    "\n",
    "# Try prompt engineering if you are not satisfied with the output\n",
    "sar_to_non_sar_question = (\n",
    "    \"Here is a list of sarcastic headlines. Read all of them in order. \"\n",
    "    \"Convert each of them to a non-sarcastic headline while preserving the original \"\n",
    "    \"meaning as much as possible. The format of your response should exactly be a \"\n",
    "    \"string that can be parsed by json to a python list of strings, without any \"\n",
    "    \"additional comments.\"\n",
    ")\n",
    "non_sar_to_sar_question = (\n",
    "    \"Here is a list of non-sarcastic headlines. Read all of them in order. \"\n",
    "    \"Convert each of them to a sarcastic headline while preserving the original \"\n",
    "    \"meaning as much as possible. The format of your response should exactly be a \"\n",
    "    \"string that can be parsed by json to a python list of strings, without any \"\n",
    "    \"additional comments.\"\n",
    ")\n",
    "\n",
    "non_sar_save_path = \"../data/non_sarcastic_generated.json\"\n",
    "non_sar_backup_path = \"../data/non_sarcastic_generated_backup.json\"\n",
    "sar_save_path = \"../data/sarcastic_generated.json\"\n",
    "sar_backup_path = \"../data/sarcastic_generated_backup.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non_sarcastic_generated_backup.json'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Optional) Back up previously generated result before running the next round\n",
    "shutil.copyfile(non_sar_save_path, non_sar_backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing data found. Loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "['Apartment decorated to appear as if it reflects a well-rounded life', 'Friend who was struggling is now making some progress', 'Stephen Miller visits children in local ICE detention center after a day of speechwriting', 'Entomologists retract discovery of new spider species, realizing it was just dust and hair', \"Man's priorities are completely different from his grandfather's\"]\n",
      "Length of response: 50\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 21.45 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generation of non-sarcastic headlines. Done in batches.\n",
    "# Number of sarcastic headlines as input: 13634\n",
    "\n",
    "# Input index range: [start, end)\n",
    "start = 2000   # start index (included)\n",
    "end = 3000   # end index (excluded)\n",
    "step = 100  # batch size: number of headlines sent in a single request\n",
    "\n",
    "# Output file is FRAGILE! If a certain batch fails, check output file length \n",
    "# immediately. Set start index to be exactly output file length when you re-run \n",
    "# this cell to avoid repeated feed of the same input.\n",
    "if get_output_file_length(non_sar_save_path) != start:\n",
    "    raise ValueError(\"Start index is set incorrectly.\")\n",
    "\n",
    "running_time = generate_headlines(sarcastic_headlines, non_sar_save_path, start, end, step, llm_auth, model, sar_to_non_sar_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that length of updated json file is as expected\n",
    "get_output_file_length(non_sar_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.54799795150757"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance check\n",
    "running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save performance check results\n",
    "# Known issue: always need to save twice and remove the first save,\n",
    "# as the first will only replicate the previous result.\n",
    "f = open('query_perf.txt', 'a')\n",
    "f.write(f\"count: 300; batch size: 100; time: {running_time}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''___________Below are for generation of sarcastic headlines___________'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sarcastic_generated_backup.json'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Optional) Back up previously generated result before running the next round\n",
    "shutil.copyfile(sar_save_path, sar_backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found. Will create new file when writing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:34<05:13, 34.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "['Oh Great, Congress Finally Nails Gender and Racial Equality', 'Because We All Needed More Veggie Recipes', 'What a Shocker: My White Inheritance', 'Taxes Are So Relaxing, Here Are 5 Ways to Make Them Less Stressful', 'Groundbreaking News: Lots of Parents Know This Scenario']\n",
      "Length of response: 100\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 34.79 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:16<09:54, 74.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "[\"'sleeping on it' really does solve all your problems, because who needs effort?\", 'toeing the race line: because labels are the most important thing in life.', \"how an essay on 'sexual paranoia' caused a frenzy at northwestern university, because academia is so calm otherwise.\", 'my disastrous search for the perfect swimsuit, because swimsuits are the pinnacle of human achievement.', '10 big space-saving ideas for small kitchens, because who doesn’t love a cluttered countertop?']\n",
      "Length of response: 100\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 101.90 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:44<09:22, 80.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "['Oh great, another riveting stage door story: Wiesenthal', 'Because aging in prison is just a walk in the park', \"Gwen Stefani bares her soul, because that's what the AMAs are for\", 'Scott Pruitt gets yet another glowing profile, because why not?', 'Death to shoppers? Al-Shabaab really knows how to make a statement']\n",
      "Length of response: 100\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 87.76 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [05:06<08:06, 81.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "['Oh sure, mom pulling a gun on teens threatening her son is totally normal parenting', 'HuffPollster: Sorry Bernie fans, but a Sanders comeback is as likely as a snowball in hell', 'I cannot do this alone: Because the Down Syndrome community just loves being dependent on allies', 'The Grid: Because what the world really needs is AI-designed websites for everyone', 'City offers free pot for the poor: Because nothing solves poverty like a good high']\n",
      "Length of response: 100\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 81.97 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [06:26<06:42, 80.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "[\"Jenna Fischer finally spills the groundbreaking secret of what Pam told Michael in his 'Office' goodbye episode\", \"Some of Amazon's suitors have absolutely no regrets about their past decisions\", \"Donald Trump just can't stop promoting his own perfect health\", '13 totally original questions you’ve never thought to ask when hiring a web design company', 'Twitter: A paradise for perpetrators and a nightmare for sexual violence survivors']\n",
      "Length of response: 100\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 79.77 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [07:16<04:41, 70.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "[\"Oh great, the FDA is finally getting around to caring about women's sexual health\", 'Your nightly routine is definitely not aging you, says no one ever', \"Thousands of people are just casually strolling to demand the Nicaraguan president's resignation\", 'The Republican debate in Utah is canceled because who needs democracy anyway?', 'A drug company prioritizing work-life balance over profit? Groundbreaking.']\n",
      "Length of response: 100\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 50.79 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [08:06<03:10, 63.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "['Oh great, another round of side-splitting parental humor', \"Finally, the 'Broad City' scene we’ve all been desperately missing\", 'Kelly Rowland’s groundbreaking advice for moms-to-be, because no one else has ever shared that', \"Because nothing says 'decency' like politics today\", 'Nothing like sneaking a cybersecurity bill into a spending package, very classy']\n",
      "Length of response: 100\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 49.39 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [09:05<02:04, 62.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "['Oh sure, the school bus is totally the last bastion of tech-free paradise', 'Shocking: Russia apparently tried to cause chaos in 2016 elections, says Senate committee', 'Wow, Sarah Palin is absolutely crystal clear in her message—what even is she saying?', 'Christie campaign donors generously find a new home for their money: his super PAC', 'McConnell assures us Republicans have enough votes for the tax bill—what a surprise']\n",
      "Length of response: 100\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 58.99 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [10:06<01:01, 61.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "[\"Oh sure, for clean air and a safe climate future, because *that's* totally under control\", 'How Nebraska can return to college football greatness—just sprinkle some magic dust!', 'Leaked report: Jerusalem at boiling point, because who doesn’t love a good simmering conflict?', 'Here’s a brand new thing you didn’t know about ‘The Office’—because we definitely needed *one more*', \"Maxine Waters to women’s convention: Trump is 'most dishonorable and despicable' president ever—shocking, right?\"]\n",
      "Length of response: 100\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 61.48 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:20<00:00, 68.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of response:\n",
      "['Oh great, using new technology to give voice to the voiceless, as if that hasn’t been tried before', 'Will Ferrell is *so* thrilled about the USA-Germany game, it’s not like he’s a comedian or anything', 'Good girls have abortions too, because obviously nobody thought of that before', 'Democrats split over opposing a government funding bill that doesn’t protect Dreamers—who could have guessed?', 'Bill Paxton discovers his revolutionary past on ‘Who Do You Think You Are?’—riveting stuff']\n",
      "Length of response: 100\n",
      "Run 'pop_buffer()' after checking response.\n",
      "Request took 73.38 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generation of sarcastic headlines. Done in batches.\n",
    "# Number of non-sarcastic headlines as input: 14985\n",
    "\n",
    "# Input index range: [start, end)\n",
    "start = 0   # start index (included)\n",
    "end = 1000   # end index (excluded)\n",
    "step = 100  # batch size: number of headlines sent in a single request\n",
    "\n",
    "# Output file is FRAGILE! If a certain batch fails, check output file length \n",
    "# immediately. Set start index to be exactly output file length when you re-run \n",
    "# this cell to avoid repeated feed of the same input.\n",
    "if get_output_file_length(sar_save_path) != start:\n",
    "    raise ValueError(\"Start index is set incorrectly.\")\n",
    "\n",
    "running_time = generate_headlines(non_sarcastic_headlines, sar_save_path, start, end, step, llm_auth, model, non_sar_to_sar_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that length of updated json file is as expected\n",
    "get_output_file_length(sar_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680.2946770191193"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance check\n",
    "running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lesson for urban cities: how chicagoans stand up for quality schools'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_sarcastic_headlines[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lesson for urban cities: how Chicagoans stand up for quality schools—because cities are easy to fix'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(sar_save_path, 'r')\n",
    "data = f.read()\n",
    "sar_gen = json.loads(data)\n",
    "sar_gen[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
