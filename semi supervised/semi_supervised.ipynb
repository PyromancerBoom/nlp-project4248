{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Atharva Tyagi\\Documents\\Development\\CS4248\\nlp-project4248\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import DatasetDict, Dataset, concatenate_datasets, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0  labels                                           headline\n",
      "0           620       0                         side effects sound awesome\n",
      "1          3523       1                       nation ready for its din din\n",
      "2          8540       1  pope francis wearing sweater vestments he got ...\n",
      "3          8530       1  nbc unveils on screen graphic informing audien...\n",
      "4          5747       0    child baffled by stationary, non-violent images\n",
      "..          ...     ...                                                ...\n",
      "595        3377       0  stan lee, creator of beloved marvel character ...\n",
      "596        5823       0  pfizer mercifully puts down another batch of t...\n",
      "597       10116       0  cern researchers apologize for destruction of ...\n",
      "598        4870       0  'just take it slow, and you'll be fine,' drunk...\n",
      "599         854       0  vince gilligan's brain spoils final season of ...\n",
      "\n",
      "[600 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sarcasm_classifications_labeled.csv\")\n",
    "\n",
    "label_column = \"labels\"  \n",
    "\n",
    "print(df)\n",
    "\n",
    "train_df, testval_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=df[label_column]\n",
    ")\n",
    "\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    testval_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=testval_df[label_column]\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'google-bert/bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"headline\"], truncation=True)\n",
    "    return tokenized_inputs\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1_score = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    f1_metrics = f1_score.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    \n",
    "    # Combine metrics into a single dictionary\n",
    "    metrics = {\"accuracy\": accuracy_score[\"accuracy\"], \"f1\": f1_metrics[\"f1\"]}\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Atharva Tyagi\\Documents\\Development\\CS4248\\nlp-project4248\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 1/5 ===\n",
      "Training set size: 420\n",
      "Unlabeled pool size: 13034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 420/420 [00:00<00:00, 41480.83 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 33450.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:00<00:00, 30004.56 examples/s]\n",
      "C:\\Users\\Atharva Tyagi\\AppData\\Local\\Temp\\ipykernel_27132\\1137105012.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/270 02:32 < 00:17, 1.58 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.046639</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.373794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.953425</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.499744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.885301</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.625022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.797535</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.655556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.801717</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.636598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.832565</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.676282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.811470</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.712248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.878357</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.684969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.880152</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.703133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6555555555555556, 'f1': 0.6525573192239859}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7521 high confidence predictions\n",
      "      Unnamed: 0  labels                                           headline\n",
      "0            620       0                         side effects sound awesome\n",
      "1           3523       1                       nation ready for its din din\n",
      "2           8540       1  pope francis wearing sweater vestments he got ...\n",
      "3           8530       1  nbc unveils on screen graphic informing audien...\n",
      "4           5747       0    child baffled by stationary, non-violent images\n",
      "...          ...     ...                                                ...\n",
      "8116       13619       2  clinton reminds new yorkers she moved there ho...\n",
      "8117       13620       2  senate: 'renewed fisa legislation imperative i...\n",
      "8118       13624       2  'entertainment weekly' critic lets director re...\n",
      "8119       13627       2  congressman picked last for committee on youth...\n",
      "8120       13628       1            grandmother doesn't care for new priest\n",
      "\n",
      "[8121 rows x 3 columns]\n",
      "Added 7521 samples to training set\n",
      "New training set size: 5684\n",
      "Remaining unlabeled pool: 5513\n",
      "\n",
      "=== Iteration 2/5 ===\n",
      "Training set size: 5684\n",
      "Unlabeled pool size: 5513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5684/5684 [00:00<00:00, 73463.20 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1218/1218 [00:00<00:00, 73302.37 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1219/1219 [00:00<00:00, 69522.00 examples/s]\n",
      "C:\\Users\\Atharva Tyagi\\AppData\\Local\\Temp\\ipykernel_27132\\1137105012.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2848' max='3560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2848/3560 31:29 < 07:52, 1.51 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225276</td>\n",
       "      <td>0.924466</td>\n",
       "      <td>0.917757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>0.242558</td>\n",
       "      <td>0.934319</td>\n",
       "      <td>0.928916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.248381</td>\n",
       "      <td>0.946634</td>\n",
       "      <td>0.941155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.289636</td>\n",
       "      <td>0.946634</td>\n",
       "      <td>0.942121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.301681</td>\n",
       "      <td>0.950739</td>\n",
       "      <td>0.946163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.312889</td>\n",
       "      <td>0.953202</td>\n",
       "      <td>0.949351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.357060</td>\n",
       "      <td>0.947455</td>\n",
       "      <td>0.942823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.337202</td>\n",
       "      <td>0.951560</td>\n",
       "      <td>0.947201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9474979491386383, 'f1': 0.9462882260066398}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5513/5513 [00:00<00:00, 34747.85 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5224 high confidence predictions\n",
      "       Unnamed: 0  labels                                           headline\n",
      "0             620       0                         side effects sound awesome\n",
      "1            3523       1                       nation ready for its din din\n",
      "2            8540       1  pope francis wearing sweater vestments he got ...\n",
      "3            8530       1  nbc unveils on screen graphic informing audien...\n",
      "4            5747       0    child baffled by stationary, non-violent images\n",
      "...           ...     ...                                                ...\n",
      "13340       13626       1                area eyesore also a data technician\n",
      "13341       13629       0  polish rapper under fire for use of the word '...\n",
      "13342       13630       0       jews to celebrate rosh hashasha or something\n",
      "13343       13632       0  mars probe destroyed by orbiting spielberg-gat...\n",
      "13344       13633       1                 dad clarifies this not a food stop\n",
      "\n",
      "[13345 rows x 3 columns]\n",
      "Added 5224 samples to training set\n",
      "New training set size: 9341\n",
      "Remaining unlabeled pool: 289\n",
      "\n",
      "=== Iteration 3/5 ===\n",
      "Training set size: 9341\n",
      "Unlabeled pool size: 289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9341/9341 [00:00<00:00, 76685.63 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2002/2002 [00:00<00:00, 8042.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2002/2002 [00:00<00:00, 68417.34 examples/s]\n",
      "C:\\Users\\Atharva Tyagi\\AppData\\Local\\Temp\\ipykernel_27132\\1137105012.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2336' max='5840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2336/5840 25:49 < 38:46, 1.51 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.718300</td>\n",
       "      <td>0.304761</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.885349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.305100</td>\n",
       "      <td>0.335838</td>\n",
       "      <td>0.890609</td>\n",
       "      <td>0.889663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.170500</td>\n",
       "      <td>0.435092</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.888380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>0.534725</td>\n",
       "      <td>0.889111</td>\n",
       "      <td>0.888234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8881118881118881, 'f1': 0.8881503178690875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 289/289 [00:00<00:00, 11718.65 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 198 high confidence predictions\n",
      "       Unnamed: 0  labels                                           headline\n",
      "0             620       0                         side effects sound awesome\n",
      "1            3523       1                       nation ready for its din din\n",
      "2            8540       1  pope francis wearing sweater vestments he got ...\n",
      "3            8530       1  nbc unveils on screen graphic informing audien...\n",
      "4            5747       0    child baffled by stationary, non-violent images\n",
      "...           ...     ...                                                ...\n",
      "13538       13514       1       anteater to lay off the fire ants for awhile\n",
      "13539       13539       1  single woman has facebook profile picture with...\n",
      "13540       13549       2  houston residents begin surveying damage of 20...\n",
      "13541       13559       0  sweatshirt string emerges triumphant after har...\n",
      "13542       13579       2  tyson holds contest to let fans submit new ide...\n",
      "\n",
      "[13543 rows x 3 columns]\n",
      "Added 198 samples to training set\n",
      "New training set size: 9480\n",
      "Remaining unlabeled pool: 91\n",
      "\n",
      "=== Iteration 4/5 ===\n",
      "Training set size: 9480\n",
      "Unlabeled pool size: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9480/9480 [00:00<00:00, 76514.09 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2031/2031 [00:00<00:00, 69504.10 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2032/2032 [00:00<00:00, 72027.73 examples/s]\n",
      "C:\\Users\\Atharva Tyagi\\AppData\\Local\\Temp\\ipykernel_27132\\1137105012.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4151' max='5930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4151/5930 46:20 < 19:52, 1.49 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.685200</td>\n",
       "      <td>0.379370</td>\n",
       "      <td>0.854751</td>\n",
       "      <td>0.856939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.302800</td>\n",
       "      <td>0.361459</td>\n",
       "      <td>0.883801</td>\n",
       "      <td>0.883069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.468901</td>\n",
       "      <td>0.879862</td>\n",
       "      <td>0.879319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.477799</td>\n",
       "      <td>0.899557</td>\n",
       "      <td>0.899165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.507483</td>\n",
       "      <td>0.901034</td>\n",
       "      <td>0.900973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.650021</td>\n",
       "      <td>0.894633</td>\n",
       "      <td>0.894475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.605080</td>\n",
       "      <td>0.896110</td>\n",
       "      <td>0.895450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9015748031496063, 'f1': 0.9016881896522877}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:00<00:00, 4339.91 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79 high confidence predictions\n",
      "       Unnamed: 0  labels                                           headline\n",
      "0             620       0                         side effects sound awesome\n",
      "1            3523       1                       nation ready for its din din\n",
      "2            8540       1  pope francis wearing sweater vestments he got ...\n",
      "3            8530       1  nbc unveils on screen graphic informing audien...\n",
      "4            5747       0    child baffled by stationary, non-violent images\n",
      "...           ...     ...                                                ...\n",
      "13617       12691       2  epa urges flint residents to stop dumping tap ...\n",
      "13618       12927       0           jay-z gives shout-out to his shareholdaz\n",
      "13619       12992       2  everyone who started watching 'mad money' in 2...\n",
      "13620       13402       1  school principal pauses for applause that neve...\n",
      "13621       13522       1  rapper not entirely sure who else is on this t...\n",
      "\n",
      "[13622 rows x 3 columns]\n",
      "Added 79 samples to training set\n",
      "New training set size: 9535\n",
      "Remaining unlabeled pool: 12\n",
      "\n",
      "=== Iteration 5/5 ===\n",
      "Training set size: 9535\n",
      "Unlabeled pool size: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9535/9535 [00:00<00:00, 65950.02 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2043/2043 [00:00<00:00, 64196.13 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2044/2044 [00:00<00:00, 65783.92 examples/s]\n",
      "C:\\Users\\Atharva Tyagi\\AppData\\Local\\Temp\\ipykernel_27132\\1137105012.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3576' max='5960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3576/5960 39:26 < 26:18, 1.51 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.748800</td>\n",
       "      <td>0.333233</td>\n",
       "      <td>0.881547</td>\n",
       "      <td>0.880382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.314600</td>\n",
       "      <td>0.312645</td>\n",
       "      <td>0.886442</td>\n",
       "      <td>0.885246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.399133</td>\n",
       "      <td>0.902594</td>\n",
       "      <td>0.901277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.448541</td>\n",
       "      <td>0.910915</td>\n",
       "      <td>0.909386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.519611</td>\n",
       "      <td>0.902594</td>\n",
       "      <td>0.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.568793</td>\n",
       "      <td>0.907978</td>\n",
       "      <td>0.905320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8977495107632094, 'f1': 0.8982259244143106}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 799.97 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 high confidence predictions\n",
      "       Unnamed: 0  labels                                           headline\n",
      "0             620       0                         side effects sound awesome\n",
      "1            3523       1                       nation ready for its din din\n",
      "2            8540       1  pope francis wearing sweater vestments he got ...\n",
      "3            8530       1  nbc unveils on screen graphic informing audien...\n",
      "4            5747       0    child baffled by stationary, non-violent images\n",
      "...           ...     ...                                                ...\n",
      "13628        5183       2  houghton mifflin harcourt releases new leather...\n",
      "13629        5273       0  'that's it? what the heck was that?' says dad ...\n",
      "13630        6804       2  new hampshire passes law forcing old people to...\n",
      "13631        8279       0  special pull-out section: rural illinois' sexi...\n",
      "13632        8612       2  'okay, gene, let's just get through this,' mar...\n",
      "\n",
      "[13633 rows x 3 columns]\n",
      "Added 11 samples to training set\n",
      "New training set size: 9543\n",
      "Remaining unlabeled pool: 1\n"
     ]
    }
   ],
   "source": [
    "def semi_supervised_training_loop(\n",
    "    tokenizer, \n",
    "    dataset_dict,\n",
    "    unlabeled_dataset,\n",
    "    original_df,\n",
    "    num_iterations=5,\n",
    "    confidence_threshold=0.8,\n",
    "):\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\n=== Iteration {iteration+1}/{num_iterations} ===\")\n",
    "        print(f\"Training set size: {len(dataset_dict['train'])}\")\n",
    "        print(f\"Unlabeled pool size: {len(unlabeled_dataset)}\")\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        tokenized_data = dataset_dict.map(preprocess_function, batched=True)\n",
    "        \n",
    "        # Train model\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_data['train'],\n",
    "            eval_dataset=tokenized_data['validation'],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "\n",
    "        # Get metrics on test set\n",
    "        predictions = trainer.predict(tokenized_data[\"test\"])\n",
    "\n",
    "        logits = predictions.predictions\n",
    "        labels = predictions.label_ids\n",
    "\n",
    "        metrics = compute_metrics((logits, labels))\n",
    "        print(metrics)\n",
    "        \n",
    "        # If no unlabeled data left, break\n",
    "        if len(unlabeled_dataset) == 0:\n",
    "            break\n",
    "        \n",
    "        # Tokenize unlabeled data\n",
    "        tokenized_unlabeled = unlabeled_dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = trainer.predict(tokenized_unlabeled)\n",
    "        logits = outputs.predictions\n",
    "        probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        confidence = np.max(probs, axis=1)\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        \n",
    "        # Select high confidence samples\n",
    "        high_conf_indices = np.where(confidence >= confidence_threshold)[0]\n",
    "        \n",
    "        if len(high_conf_indices) == 0:\n",
    "            print(\"No high confidence predictions found. Try lowering the threshold.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Found {len(high_conf_indices)} high confidence predictions\")\n",
    "        \n",
    "        # Add high confidence samples to training set\n",
    "        new_labeled_samples = unlabeled_dataset.select(high_conf_indices)\n",
    "        \n",
    "        # Remove selected samples from unlabeled pool\n",
    "        remaining_indices = [i for i in range(len(unlabeled_dataset)) if i not in high_conf_indices]\n",
    "        unlabeled_dataset = unlabeled_dataset.select(remaining_indices)\n",
    "\n",
    "        new_samples_df = new_labeled_samples.to_pandas()\n",
    "        new_samples_df['labels'] = predictions[high_conf_indices]\n",
    "\n",
    "        original_df = pd.concat([original_df, new_samples_df], ignore_index=True)\n",
    "\n",
    "        print(original_df)\n",
    "\n",
    "        train_df, testval_df = train_test_split(\n",
    "            original_df, \n",
    "            test_size=0.3, \n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "\n",
    "        val_df, test_df = train_test_split(\n",
    "            testval_df,\n",
    "            test_size=0.5,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "        dataset_dict = DatasetDict({\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset,\n",
    "            'test': test_dataset\n",
    "        })\n",
    "        \n",
    "        print(f\"Added {len(new_labeled_samples)} samples to training set\")\n",
    "        print(f\"New training set size: {len(dataset_dict['train'])}\")\n",
    "        print(f\"Remaining unlabeled pool: {len(unlabeled_dataset)}\")\n",
    "    \n",
    "    return dataset_dict, unlabeled_dataset, model\n",
    "\n",
    "unlabeled_dataset = load_dataset(\"csv\", data_files=\"sarcasm_classifications.csv\")[\"train\"]\n",
    "\n",
    "# Run the loop\n",
    "final_dataset_dict, remaining_unlabeled, final_model = semi_supervised_training_loop(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_dict=dataset_dict,\n",
    "    unlabeled_dataset=unlabeled_dataset,\n",
    "    original_df=df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 624.98ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "761635"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset_dict['train'].to_csv('semi_supervised_annotation/results_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 750.46ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "164051"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset_dict['validation'].to_csv('semi_supervised_annotation/results_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 750.10ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "164026"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset_dict['test'].to_csv('semi_supervised_annotation/results_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
